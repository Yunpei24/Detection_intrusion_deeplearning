{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Autheur: JOSHUA JUSTE EMMANUEL YUN PEI NIKIEMA\n","## Classe: INGC3 InDIA\n","## Date: Jeudi 11 Avril 2024"]},{"cell_type":"markdown","metadata":{},"source":["# Importation des librairies"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:24:13.722418Z","iopub.status.busy":"2024-04-11T01:24:13.722058Z","iopub.status.idle":"2024-04-11T01:24:31.009620Z","shell.execute_reply":"2024-04-11T01:24:31.008805Z","shell.execute_reply.started":"2024-04-11T01:24:13.722388Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["2024-04-11 01:24:21.790826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-04-11 01:24:21.790927: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-04-11 01:24:21.933293: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"]}],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import torch  # Import de la bibliothèque PyTorch pour le deep learning\n","from torch.utils.data import TensorDataset, DataLoader  # Pour les ensembles de données et les chargeurs de données\n","import numpy as np  # Pour les opérations numériques\n","from sklearn.pipeline import Pipeline  # Pour créer un pipeline de prétraitement des données\n","from sklearn.preprocessing import StandardScaler, LabelEncoder  # Pour le prétraitement des données\n","from sklearn.model_selection import train_test_split  # Pour la séparation des données en ensembles d'entraînement et de test\n","from sklearn.impute import SimpleImputer  # Pour remplir les valeurs manquantes dans les données\n","from sklearn.pipeline import Pipeline\n","import torch.nn.functional as F  # Pour les fonctions d'activation et les fonctions de perte\n","from torch.utils.tensorboard import SummaryWriter  # Pour la visualisation des résultats avec TensorBoard"]},{"cell_type":"markdown","metadata":{},"source":["# Création de la Classe pour le prétraitement des données"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:24:31.012407Z","iopub.status.busy":"2024-04-11T01:24:31.011474Z","iopub.status.idle":"2024-04-11T01:24:31.023320Z","shell.execute_reply":"2024-04-11T01:24:31.022282Z","shell.execute_reply.started":"2024-04-11T01:24:31.012370Z"},"trusted":true},"outputs":[],"source":["class DataProcessor:\n","    \"\"\"Classe pour prétraiter les données.\"\"\"\n","\n","    def __init__(self, device):\n","        \"\"\"Initialiseur de la classe DataProcessor.\"\"\"\n","        self.encoder = LabelEncoder()\n","        self.preprocessing_pipeline = Pipeline(steps=[\n","            ('imputer', SimpleImputer()),  # Remplace les valeurs manquantes\n","            ('scaler', StandardScaler())   # Met à l'échelle les caractéristiques\n","        ])\n","        self.device = device\n","\n","    \n","    def preprocess_data(self, data, target_name):\n","        \"\"\"Prétraite les données en remplissant les valeurs manquantes, en mettant à l'échelle et en encodant la variable cible.\n","\n","        Args:\n","            data (pandas.DataFrame): Le DataFrame contenant les données à prétraiter.\n","            target_name (str): Le nom de la colonne contenant la variable cible.\n","\n","        Returns:\n","            torch.Tensor, torch.Tensor: Les données prétraitées (caractéristiques) et la variable cible encodée.\n","        \"\"\"\n","\n","        target = data[target_name]\n","        data = data.drop(columns=[target_name])\n","        data = data.replace(to_replace=[np.Inf, -np.Inf], value=np.nan)\n","        \n","        # Conversion des colonnes en int ou float\n","        for col in data.columns:\n","            try:\n","                data[col] = pd.to_numeric(data[col], errors='coerce')  # Tente une conversion numérique générale\n","            except ValueError:\n","                pass\n","        \n","        data = data.replace(to_replace=[np.Inf, -np.Inf], value=np.nan)\n","\n","        # Conversion en tenseur et transfert sur le périphérique approprié (GPU ou CPU)\n","        data_tensor = torch.from_numpy(data.values).float().to(self.device)\n","        target_tensor = torch.from_numpy(self.encoder.fit_transform(target)).long().to(self.device)\n","\n","        # Application du pipeline de prétraitement\n","        preprocessed_data = self.preprocessing_pipeline.fit_transform(data_tensor.cpu().numpy())\n","        preprocessed_data_tensor = torch.from_numpy(preprocessed_data).float().to(self.device)\n","\n","        return preprocessed_data_tensor, target_tensor\n"]},{"cell_type":"markdown","metadata":{},"source":["# Création de la Classe pour Gérer les Chargeurs des données"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:24:31.025035Z","iopub.status.busy":"2024-04-11T01:24:31.024678Z","iopub.status.idle":"2024-04-11T01:24:31.052792Z","shell.execute_reply":"2024-04-11T01:24:31.051890Z","shell.execute_reply.started":"2024-04-11T01:24:31.025003Z"},"trusted":true},"outputs":[],"source":["class DataLoaderManager:\n","    \"\"\"\n","    Classe pour gérer la création et la configuration des chargeurs de données pour l'entraînement et l'évaluation de modèles.\n","    \"\"\"\n","\n","    def __init__(self, batch_size_train=100, batch_size_test=20, device=None):\n","        \"\"\"\n","        Initialise la classe DataLoaderManager.\n","\n","        Args:\n","            batch_size_train (int, optional): Taille des lots pour l'entraînement. Par défaut, 100.\n","            batch_size_test (int, optional): Taille des lots pour l'évaluation. Par défaut, 20.\n","            device (torch.device, optional): Périphérique utilisé pour le calcul (CPU ou GPU). \n","                                            Si None, le périphérique sera déterminé automatiquement.\n","        \"\"\"\n","\n","        self.batch_size_train = 100\n","        self.batch_size_test = 20\n","        self.device = device # Détermine le périphérique\n","\n","    def create_datasets_and_loaders(self, X_train, X_test, y_train, y_test):\n","        \"\"\"\n","        Crée des ensembles de données et des chargeurs de données pour l'entraînement et l'évaluation.\n","\n","        Args:\n","            X_train (np.ndarray): Données d'entraînement (caractéristiques).\n","            X_test (np.ndarray): Données de test (caractéristiques).\n","            y_train (np.ndarray): Étiquettes d'entraînement.\n","            y_test (np.ndarray): Étiquettes de test.\n","\n","        Returns:\n","            tuple: Quatre objets :\n","                - trainset (TensorDataset): Ensemble de données d'entraînement.\n","                - testset (TensorDataset): Ensemble de données de test.\n","                - trainloader (DataLoader): Chargeur de données pour l'entraînement.\n","                - testloader (DataLoader): Chargeur de données pour l'évaluation.\n","        \"\"\"\n","\n","        # transfert sur le périphérique approprié\n","        X_train, y_train = X_train.to(self.device), y_train.to(self.device)\n","        X_test, y_test = X_test.to(self.device), y_test.to(self.device)\n","\n","        # Création des ensembles de données\n","        trainset = TensorDataset(X_train, y_train)\n","        testset = TensorDataset(X_test, y_test)\n","\n","        # Création des chargeurs de données\n","        trainloader = DataLoader(dataset=trainset, batch_size=self.batch_size_train, shuffle=True)\n","        testloader = DataLoader(dataset=testset, batch_size=self.batch_size_test, shuffle=True)\n","\n","        return trainset, testset, trainloader, testloader\n"]},{"cell_type":"markdown","metadata":{},"source":["# Création de la Classe de notre Modèle"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:24:31.055047Z","iopub.status.busy":"2024-04-11T01:24:31.054642Z","iopub.status.idle":"2024-04-11T01:24:31.067729Z","shell.execute_reply":"2024-04-11T01:24:31.066860Z","shell.execute_reply.started":"2024-04-11T01:24:31.055022Z"},"trusted":true},"outputs":[],"source":["class IntrusionDetectionModel(torch.nn.Module):\n","    \"\"\"Classe pour le modèle de détection d'intrusion.\"\"\"\n","\n","    def __init__(self, in_features, out_features, device):\n","        \"\"\"Initialiseur de la classe IntrusionDetectionModel.\"\"\"\n","        super().__init__()\n","        self.fc1 = torch.nn.Linear(in_features, 50)\n","        self.fc2 = torch.nn.Linear(50, 100)\n","        self.fc3 = torch.nn.Linear(100, out_features)\n","        self.device = device\n","        self.to(self.device)\n","\n","    def forward(self, x):\n","        \"\"\"Méthode de propagation avant.\"\"\"\n","        x = F.relu(self.fc1(x))\n","        x = F.relu(self.fc2(x))\n","        x = self.fc3(x)\n","        return x\n"]},{"cell_type":"markdown","metadata":{},"source":["# Création d'une fonction pour charger et transformer les données en DataFrame"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:24:31.069400Z","iopub.status.busy":"2024-04-11T01:24:31.069003Z","iopub.status.idle":"2024-04-11T01:24:31.082531Z","shell.execute_reply":"2024-04-11T01:24:31.081664Z","shell.execute_reply.started":"2024-04-11T01:24:31.069370Z"},"trusted":true},"outputs":[],"source":["\n","import psutil\n","\n","def load_csv_files_progressively(directory, memory_threshold=70):\n","    \"\"\"\n","    Charge tous les fichiers CSV dans un répertoire donné et les combine progressivement en un unique DataFrame.\n","\n","    Args:\n","    - directory (str): Chemin du répertoire contenant les fichiers CSV.\n","    - memory_threshold (int): Seuil d'utilisation de la mémoire RAM en pourcentage pour arrêter la concaténation.\n","\n","    Returns:\n","    - DataFrame: DataFrame combinant toutes les données des fichiers CSV progressivement.\n","    - int: Nombre de DataFrames concaténés.\n","    \"\"\"\n","\n","    # Liste pour stocker les DataFrames combinés progressivement\n","    combined_dfs = []\n","    file_to_exclude = ['02-20-2018.csv', '03-01-2018.csv', '02-16-2018.csv']\n","\n","    # Parcours des fichiers dans le répertoire\n","    files = [file for file in os.listdir(directory) if file.endswith('.csv') and file not in file_to_exclude]\n","\n","\n","    # Surveillance de l'utilisation de la mémoire\n","    mem = psutil.virtual_memory()\n","\n","    # Lecture et concaténation progressive des fichiers\n","    for i, file in enumerate(files):\n","        print('Fichier utilisé : ', file)\n","        file_path = os.path.join(directory, file)\n","        df = pd.read_csv(file_path)\n","        df = df.drop(columns=[\"Timestamp\"], axis=1)\n","\n","        if i == 0:\n","            combined_df = df\n","        else:\n","            combined_df = pd.concat([combined_df, df], ignore_index=True)\n","\n","\n","        # Vérifier l'utilisation de la mémoire\n","        mem = psutil.virtual_memory()\n","        if mem.percent >= memory_threshold:\n","            break\n","\n","    # Retourner le DataFrame combiné progressivement et le nombre de DataFrames concaténés\n","    return combined_df, i + 1\n"]},{"cell_type":"markdown","metadata":{},"source":["# Création de la fonction de la boucle d'entrainement"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:24:31.085316Z","iopub.status.busy":"2024-04-11T01:24:31.084743Z","iopub.status.idle":"2024-04-11T01:24:31.100362Z","shell.execute_reply":"2024-04-11T01:24:31.099636Z","shell.execute_reply.started":"2024-04-11T01:24:31.085285Z"},"trusted":true},"outputs":[],"source":["def train_model(model, criterion, optimizer, trainloader, max_iter, writer, device):\n","    \"\"\"\n","    Fonction pour entraîner le modèle et enregistrer les pertes dans un fichier.\n","\n","    Args:\n","        model (torch.nn.Module): Le modèle à entraîner.\n","        criterion (torch.nn.Module): La fonction de perte à utiliser pour calculer la perte.\n","        optimizer (torch.optim.Optimizer): L'optimiseur à utiliser pour mettre à jour les poids du modèle.\n","        trainloader (torch.utils.data.DataLoader): Le chargeur de données pour les données d'entraînement.\n","        max_iter (int): Le nombre total d'itérations d'entraînement (epochs).\n","        writer (SummaryWriter): L'écrivain Tensorboard pour enregistrer les statistiques d'entraînement.\n","        device (torch.device): Le périphérique d'exécution (GPU ou CPU).\n","\n","    Returns:\n","        list: Une liste contenant les pertes moyennes à chaque époque.\n","    \"\"\"\n","\n","    model.to(device)  # Transférer le modèle sur le GPU s'il est disponible\n","    model.train()  # Mettre le modèle en mode d'entraînement\n","\n","    losses = []  # Liste pour stocker les pertes moyennes à chaque époque\n","\n","    for epoch in range(max_iter):  # Boucle sur le nombre total d'itérations d'entraînement (epochs)\n","        running_loss = 0  # Initialiser la perte courante à zéro\n","\n","        for i, (data, target) in enumerate(trainloader):  # Boucle sur les données d'entraînement\n","            data, target = data.to(device), target.to(device)  # Transférer les données sur le GPU s'il est disponible\n","            optimizer.zero_grad()  # Réinitialiser les gradients à zéro\n","            pred = model(data)  # Passer les données dans le modèle pour obtenir les prédictions\n","            loss = criterion(pred, target.long())  # Calculer la perte\n","            loss.backward()  # Calculer les gradients par rétropropagation\n","            optimizer.step()  # Mettre à jour les poids du modèle\n","            running_loss += loss.item()  # Ajouter la perte à la perte courante\n","\n","            # Afficher les statistiques et les enregistrer dans Tensorboard toutes les 1000 itérations\n","            if (i % 500 == 0):\n","                print(f\"Epoch: {epoch+1} / {max_iter}, Steps: {i+1} / {len(trainloader)}, Loss: {loss.item()}\")\n","                step = epoch * len(trainloader) + i\n","                writer.add_scalar(\"Loss\", loss.item(), step)  # Enregistrer la perte dans Tensorboard\n","                writer.add_scalar(\"Mean/Loss\", running_loss/100, step)  # Enregistrer la moyenne de la perte dans Tensorboard\n","                running_loss = 0  # Réinitialiser la perte courante\n","\n","        # Calculer la perte moyenne pour l'époque en cours\n","        epoch_loss = running_loss / len(trainloader)\n","\n","        # Enregistrer la perte moyenne dans la liste\n","        losses.append(epoch_loss)\n","\n","    return losses  # Retourner la liste des pertes moyennes\n"]},{"cell_type":"markdown","metadata":{},"source":["# Création de la fonction pour l'évaluation du modèle"]},{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T02:37:47.073712Z","iopub.status.busy":"2024-04-11T02:37:47.072993Z","iopub.status.idle":"2024-04-11T02:37:47.086316Z","shell.execute_reply":"2024-04-11T02:37:47.085295Z","shell.execute_reply.started":"2024-04-11T02:37:47.073676Z"},"trusted":true},"outputs":[],"source":["import torchmetrics as tm\n","from sklearn.metrics import precision_recall_fscore_support\n","\n","def evaluate_model(model, criterion, testloader, device, Nb_class):\n","    \"\"\"\n","    Fonction pour évaluer le modèle de détection d'intrusion sur l'ensemble de données de test et\n","    enregistrer les pertes individuelles et les métriques de performance.\n","\n","    Args:\n","        model (torch.nn.Module): Le modèle de détection d'intrusion entraîné.\n","        criterion (torch.nn.Module): La fonction de perte utilisée pour l'évaluation.\n","        testloader (torch.utils.data.DataLoader): Le chargeur de données pour l'ensemble de données de test.\n","        device (torch.device): Le périphérique d'exécution (GPU ou CPU).\n","\n","    Returns:\n","        dict: Un dictionnaire contenant les métriques de performance (perte, précision, rappel, F1-score, AUC).\n","    \"\"\"\n","\n","    model.eval()  # Basculer le modèle en mode évaluation\n","\n","    losses = []  # Liste pour stocker les pertes individuelles\n","    precision_scores = []\n","    recall_scores = []\n","    f1_scores = []\n","    accuracies = []\n","\n","    with torch.no_grad():\n","        accuracy = tm.Accuracy(task='MULTICLASS', num_classes=Nb_class).to(device)  # Instancier la métrique accuracy multiclasse\n","        \n","        for i, data in enumerate(testloader):\n","            inputs, labels = data\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","\n","            losses.append(loss.item())\n","\n","            pred = outputs.argmax(dim=1)\n","            \n","            # Calculer la précision, le rappel et le F1-score à l'aide de sklearn.metrics\n","            precision, recall, f1_score, _ = precision_recall_fscore_support(\n","                labels.cpu(), pred.cpu(), average='macro', zero_division=0\n","            )\n","\n","            # Appeler les métriques instanciées\n","            accuracy(pred, labels)\n","\n","            precision_scores.append(precision)  # Récupérer la valeur via compute()\n","            recall_scores.append(recall)\n","            f1_scores.append(f1_score)\n","            accuracies.append(accuracy.compute())\n","            \n","\n","    epoch_loss = sum(losses) / len(losses)\n","    epoch_precision = sum(precision_scores) / len(precision_scores)\n","    epoch_recall = sum(recall_scores) / len(recall_scores)\n","    epoch_f1 = sum(f1_scores) / len(f1_scores)\n","    epoch_accu = sum(accuracies) / len(accuracies)\n","    \n","\n","    # Renvoyer un dictionnaire contenant les métriques\n","    metrics = {\n","        \"Test set loss\": epoch_loss,\n","        \"Accuracy means\": epoch_accu,\n","        \"Precision\": epoch_precision,\n","        \"Recall\": epoch_recall,\n","        \"F1-score\": epoch_f1\n","    }\n","    return metrics"]},{"cell_type":"code","execution_count":8,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:24:33.478636Z","iopub.status.busy":"2024-04-11T01:24:33.478288Z","iopub.status.idle":"2024-04-11T01:26:43.997502Z","shell.execute_reply":"2024-04-11T01:26:43.996665Z","shell.execute_reply.started":"2024-04-11T01:24:33.478605Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Fichier utilisé :  02-28-2018.csv\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_34/216919516.py:31: DtypeWarning: Columns (0,1,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78) have mixed types. Specify dtype option on import or set low_memory=False.\n","  df = pd.read_csv(file_path)\n"]},{"name":"stdout","output_type":"stream","text":["Fichier utilisé :  02-15-2018.csv\n","Fichier utilisé :  02-21-2018.csv\n","Fichier utilisé :  03-02-2018.csv\n","Fichier utilisé :  02-22-2018.csv\n","Fichier utilisé :  02-14-2018.csv\n","Fichier utilisé :  02-23-2018.csv\n"]}],"source":["directory = '/kaggle/input/ids-intrusion-csv/'\n","# Appel de la fonction pour charger les données\n","df, nb_file = load_csv_files_progressively(directory)"]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:26:43.998831Z","iopub.status.busy":"2024-04-11T01:26:43.998574Z","iopub.status.idle":"2024-04-11T01:26:44.003577Z","shell.execute_reply":"2024-04-11T01:26:44.002713Z","shell.execute_reply.started":"2024-04-11T01:26:43.998809Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Le nombre de fichier concatené =  7 /10\n","Shape du dataset charger =  (6904554, 79)\n"]}],"source":["## Information\n","print(\"Le nombre de fichier concatené = \", nb_file, \"/10\")\n","\n","print(\"Shape du dataset charger = \", df.shape)"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T02:37:54.107322Z","iopub.status.busy":"2024-04-11T02:37:54.106461Z","iopub.status.idle":"2024-04-11T02:37:54.130868Z","shell.execute_reply":"2024-04-11T02:37:54.129918Z","shell.execute_reply.started":"2024-04-11T02:37:54.107290Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Dst Port</th>\n","      <th>Protocol</th>\n","      <th>Flow Duration</th>\n","      <th>Tot Fwd Pkts</th>\n","      <th>Tot Bwd Pkts</th>\n","      <th>TotLen Fwd Pkts</th>\n","      <th>TotLen Bwd Pkts</th>\n","      <th>Fwd Pkt Len Max</th>\n","      <th>Fwd Pkt Len Min</th>\n","      <th>Fwd Pkt Len Mean</th>\n","      <th>...</th>\n","      <th>Fwd Seg Size Min</th>\n","      <th>Active Mean</th>\n","      <th>Active Std</th>\n","      <th>Active Max</th>\n","      <th>Active Min</th>\n","      <th>Idle Mean</th>\n","      <th>Idle Std</th>\n","      <th>Idle Max</th>\n","      <th>Idle Min</th>\n","      <th>Label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>443</td>\n","      <td>6</td>\n","      <td>94658</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>708</td>\n","      <td>3718</td>\n","      <td>387</td>\n","      <td>0</td>\n","      <td>118.0</td>\n","      <td>...</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Benign</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>443</td>\n","      <td>6</td>\n","      <td>206</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Benign</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>445</td>\n","      <td>6</td>\n","      <td>165505</td>\n","      <td>3</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Benign</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>443</td>\n","      <td>6</td>\n","      <td>102429</td>\n","      <td>6</td>\n","      <td>7</td>\n","      <td>708</td>\n","      <td>3718</td>\n","      <td>387</td>\n","      <td>0</td>\n","      <td>118.0</td>\n","      <td>...</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Benign</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>443</td>\n","      <td>6</td>\n","      <td>167</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>...</td>\n","      <td>20</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>Benign</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 79 columns</p>\n","</div>"],"text/plain":["  Dst Port Protocol Flow Duration Tot Fwd Pkts Tot Bwd Pkts TotLen Fwd Pkts  \\\n","0      443        6         94658            6            7             708   \n","1      443        6           206            2            0               0   \n","2      445        6        165505            3            1               0   \n","3      443        6        102429            6            7             708   \n","4      443        6           167            2            0               0   \n","\n","  TotLen Bwd Pkts Fwd Pkt Len Max Fwd Pkt Len Min Fwd Pkt Len Mean  ...  \\\n","0            3718             387               0            118.0  ...   \n","1               0               0               0              0.0  ...   \n","2               0               0               0              0.0  ...   \n","3            3718             387               0            118.0  ...   \n","4               0               0               0              0.0  ...   \n","\n","  Fwd Seg Size Min Active Mean Active Std Active Max Active Min Idle Mean  \\\n","0               20         0.0        0.0          0          0       0.0   \n","1               20         0.0        0.0          0          0       0.0   \n","2               20         0.0        0.0          0          0       0.0   \n","3               20         0.0        0.0          0          0       0.0   \n","4               20         0.0        0.0          0          0       0.0   \n","\n","  Idle Std Idle Max Idle Min   Label  \n","0      0.0        0        0  Benign  \n","1      0.0        0        0  Benign  \n","2      0.0        0        0  Benign  \n","3      0.0        0        0  Benign  \n","4      0.0        0        0  Benign  \n","\n","[5 rows x 79 columns]"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["# Affichage des 5 premières ligne du jeu de donnée\n","df.head()"]},{"cell_type":"code","execution_count":11,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:26:44.040603Z","iopub.status.busy":"2024-04-11T01:26:44.040251Z","iopub.status.idle":"2024-04-11T01:26:44.064327Z","shell.execute_reply":"2024-04-11T01:26:44.063455Z","shell.execute_reply.started":"2024-04-11T01:26:44.040574Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 6904554 entries, 0 to 6904553\n","Data columns (total 79 columns):\n"," #   Column             Dtype \n","---  ------             ----- \n"," 0   Dst Port           object\n"," 1   Protocol           object\n"," 2   Flow Duration      object\n"," 3   Tot Fwd Pkts       object\n"," 4   Tot Bwd Pkts       object\n"," 5   TotLen Fwd Pkts    object\n"," 6   TotLen Bwd Pkts    object\n"," 7   Fwd Pkt Len Max    object\n"," 8   Fwd Pkt Len Min    object\n"," 9   Fwd Pkt Len Mean   object\n"," 10  Fwd Pkt Len Std    object\n"," 11  Bwd Pkt Len Max    object\n"," 12  Bwd Pkt Len Min    object\n"," 13  Bwd Pkt Len Mean   object\n"," 14  Bwd Pkt Len Std    object\n"," 15  Flow Byts/s        object\n"," 16  Flow Pkts/s        object\n"," 17  Flow IAT Mean      object\n"," 18  Flow IAT Std       object\n"," 19  Flow IAT Max       object\n"," 20  Flow IAT Min       object\n"," 21  Fwd IAT Tot        object\n"," 22  Fwd IAT Mean       object\n"," 23  Fwd IAT Std        object\n"," 24  Fwd IAT Max        object\n"," 25  Fwd IAT Min        object\n"," 26  Bwd IAT Tot        object\n"," 27  Bwd IAT Mean       object\n"," 28  Bwd IAT Std        object\n"," 29  Bwd IAT Max        object\n"," 30  Bwd IAT Min        object\n"," 31  Fwd PSH Flags      object\n"," 32  Bwd PSH Flags      object\n"," 33  Fwd URG Flags      object\n"," 34  Bwd URG Flags      object\n"," 35  Fwd Header Len     object\n"," 36  Bwd Header Len     object\n"," 37  Fwd Pkts/s         object\n"," 38  Bwd Pkts/s         object\n"," 39  Pkt Len Min        object\n"," 40  Pkt Len Max        object\n"," 41  Pkt Len Mean       object\n"," 42  Pkt Len Std        object\n"," 43  Pkt Len Var        object\n"," 44  FIN Flag Cnt       object\n"," 45  SYN Flag Cnt       object\n"," 46  RST Flag Cnt       object\n"," 47  PSH Flag Cnt       object\n"," 48  ACK Flag Cnt       object\n"," 49  URG Flag Cnt       object\n"," 50  CWE Flag Count     object\n"," 51  ECE Flag Cnt       object\n"," 52  Down/Up Ratio      object\n"," 53  Pkt Size Avg       object\n"," 54  Fwd Seg Size Avg   object\n"," 55  Bwd Seg Size Avg   object\n"," 56  Fwd Byts/b Avg     object\n"," 57  Fwd Pkts/b Avg     object\n"," 58  Fwd Blk Rate Avg   object\n"," 59  Bwd Byts/b Avg     object\n"," 60  Bwd Pkts/b Avg     object\n"," 61  Bwd Blk Rate Avg   object\n"," 62  Subflow Fwd Pkts   object\n"," 63  Subflow Fwd Byts   object\n"," 64  Subflow Bwd Pkts   object\n"," 65  Subflow Bwd Byts   object\n"," 66  Init Fwd Win Byts  object\n"," 67  Init Bwd Win Byts  object\n"," 68  Fwd Act Data Pkts  object\n"," 69  Fwd Seg Size Min   object\n"," 70  Active Mean        object\n"," 71  Active Std         object\n"," 72  Active Max         object\n"," 73  Active Min         object\n"," 74  Idle Mean          object\n"," 75  Idle Std           object\n"," 76  Idle Max           object\n"," 77  Idle Min           object\n"," 78  Label              object\n","dtypes: object(79)\n","memory usage: 4.1+ GB\n"]}],"source":["df.info()"]},{"cell_type":"markdown","metadata":{},"source":["**NB :** A cause de certains fichiers notamment:\n","*     02-28-2018.csv\n","*     03-01-2018.csv\n","*     02-16-2018.csv\n","\n","Pandas n'arrive pas à déterminer les types des colonnes et donc il prend le type par defaut qui est`object`. Donc lors de preprocessing nous avons effectuer une transformation spécifique.\n"]},{"cell_type":"markdown","metadata":{},"source":["# Partie Principale"]},{"cell_type":"markdown","metadata":{},"source":["## 1. Partie d'obtention des données prêt pour l'entrainement"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:26:44.066276Z","iopub.status.busy":"2024-04-11T01:26:44.065976Z","iopub.status.idle":"2024-04-11T01:26:44.108763Z","shell.execute_reply":"2024-04-11T01:26:44.107832Z","shell.execute_reply.started":"2024-04-11T01:26:44.066251Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Exécution sur cuda\n"]}],"source":["# Définir le périphérique d'exécution (GPU ou CPU)\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Exécution sur {device}\")"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:26:44.110563Z","iopub.status.busy":"2024-04-11T01:26:44.109926Z","iopub.status.idle":"2024-04-11T01:33:09.755628Z","shell.execute_reply":"2024-04-11T01:33:09.754610Z","shell.execute_reply.started":"2024-04-11T01:26:44.110530Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Données chargées et prétraitées.\n","Données divisées en ensembles d'entraînement et de test.\n","Ensembles de données et chargeurs de données créés.\n"]}],"source":["# Charger les données CSV et les prétraiter\n","data_processor = DataProcessor(device)\n","X, y = data_processor.preprocess_data(df, \"Label\")\n","print(\"Données chargées et prétraitées.\")\n","\n","# Diviser les données en ensembles d'entraînement et de test\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","print(\"Données divisées en ensembles d'entraînement et de test.\")\n","\n","# Créer des ensembles de données et des chargeurs de données pour l'entraînement et l'évaluation\n","data_loader_manager = DataLoaderManager(device=device)\n","trainset, testset, trainloader, testloader = data_loader_manager.create_datasets_and_loaders(X_train, X_test, y_train, y_test)\n","in_features, out_features = X_train.shape[1], len(np.unique(y_train.cpu()))\n","print(\"Ensembles de données et chargeurs de données créés.\")\n"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Initialisation du modèle, de la fonction perte et de l'optimiseur"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:33:09.757243Z","iopub.status.busy":"2024-04-11T01:33:09.756925Z","iopub.status.idle":"2024-04-11T01:33:10.362269Z","shell.execute_reply":"2024-04-11T01:33:10.361300Z","shell.execute_reply.started":"2024-04-11T01:33:09.757201Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Modèle de détection d'intrusion initialisé.\n","Critère et optimiseur définis.\n"]}],"source":["# Initialiser le modèle de détection d'intrusion\n","model = IntrusionDetectionModel(in_features, out_features, device)\n","print(\"Modèle de détection d'intrusion initialisé.\")\n","\n","# Définir la fonction de perte et l'optimiseur pour l'entraînement\n","criterion = torch.nn.CrossEntropyLoss()\n","optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n","print(\"Critère et optimiseur définis.\")"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Entrainement du modèle"]},{"cell_type":"code","execution_count":15,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T01:33:10.364261Z","iopub.status.busy":"2024-04-11T01:33:10.363875Z","iopub.status.idle":"2024-04-11T01:36:51.831785Z","shell.execute_reply":"2024-04-11T01:36:51.830842Z","shell.execute_reply.started":"2024-04-11T01:33:10.364210Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Writer de Tensorboard initialisé.\n","Epoch: 1 / 2, Steps: 1 / 55237, Loss: 2.497056484222412\n","Epoch: 1 / 2, Steps: 501 / 55237, Loss: 0.5883442163467407\n","Epoch: 1 / 2, Steps: 1001 / 55237, Loss: 0.6718313694000244\n","Epoch: 1 / 2, Steps: 1501 / 55237, Loss: 0.38286054134368896\n","Epoch: 1 / 2, Steps: 2001 / 55237, Loss: 0.19746530055999756\n","Epoch: 1 / 2, Steps: 2501 / 55237, Loss: 0.22470836341381073\n","Epoch: 1 / 2, Steps: 3001 / 55237, Loss: 0.13742442429065704\n","Epoch: 1 / 2, Steps: 3501 / 55237, Loss: 0.11437146365642548\n","Epoch: 1 / 2, Steps: 4001 / 55237, Loss: 0.2636166512966156\n","Epoch: 1 / 2, Steps: 4501 / 55237, Loss: 0.11005084961652756\n","Epoch: 1 / 2, Steps: 5001 / 55237, Loss: 0.09039358794689178\n","Epoch: 1 / 2, Steps: 5501 / 55237, Loss: 0.0916852056980133\n","Epoch: 1 / 2, Steps: 6001 / 55237, Loss: 0.1253376603126526\n","Epoch: 1 / 2, Steps: 6501 / 55237, Loss: 0.05603886768221855\n","Epoch: 1 / 2, Steps: 7001 / 55237, Loss: 0.15858130156993866\n","Epoch: 1 / 2, Steps: 7501 / 55237, Loss: 0.08005820214748383\n","Epoch: 1 / 2, Steps: 8001 / 55237, Loss: 0.07187195122241974\n","Epoch: 1 / 2, Steps: 8501 / 55237, Loss: 0.14542599022388458\n","Epoch: 1 / 2, Steps: 9001 / 55237, Loss: 0.10696090757846832\n","Epoch: 1 / 2, Steps: 9501 / 55237, Loss: 0.10022199898958206\n","Epoch: 1 / 2, Steps: 10001 / 55237, Loss: 0.05807183310389519\n","Epoch: 1 / 2, Steps: 10501 / 55237, Loss: 0.06570802628993988\n","Epoch: 1 / 2, Steps: 11001 / 55237, Loss: 0.06939626485109329\n","Epoch: 1 / 2, Steps: 11501 / 55237, Loss: 0.07081326097249985\n","Epoch: 1 / 2, Steps: 12001 / 55237, Loss: 0.15581679344177246\n","Epoch: 1 / 2, Steps: 12501 / 55237, Loss: 0.0649145245552063\n","Epoch: 1 / 2, Steps: 13001 / 55237, Loss: 0.09520131349563599\n","Epoch: 1 / 2, Steps: 13501 / 55237, Loss: 0.02987242303788662\n","Epoch: 1 / 2, Steps: 14001 / 55237, Loss: 0.08410173654556274\n","Epoch: 1 / 2, Steps: 14501 / 55237, Loss: 0.16990886628627777\n","Epoch: 1 / 2, Steps: 15001 / 55237, Loss: 0.020828789100050926\n","Epoch: 1 / 2, Steps: 15501 / 55237, Loss: 0.021896202117204666\n","Epoch: 1 / 2, Steps: 16001 / 55237, Loss: 0.05965780094265938\n","Epoch: 1 / 2, Steps: 16501 / 55237, Loss: 0.05881892517209053\n","Epoch: 1 / 2, Steps: 17001 / 55237, Loss: 0.11113109439611435\n","Epoch: 1 / 2, Steps: 17501 / 55237, Loss: 0.019596748054027557\n","Epoch: 1 / 2, Steps: 18001 / 55237, Loss: 0.05499495193362236\n","Epoch: 1 / 2, Steps: 18501 / 55237, Loss: 0.017300710082054138\n","Epoch: 1 / 2, Steps: 19001 / 55237, Loss: 0.022096849977970123\n","Epoch: 1 / 2, Steps: 19501 / 55237, Loss: 0.06112513691186905\n","Epoch: 1 / 2, Steps: 20001 / 55237, Loss: 0.061448052525520325\n","Epoch: 1 / 2, Steps: 20501 / 55237, Loss: 0.057008255273103714\n","Epoch: 1 / 2, Steps: 21001 / 55237, Loss: 0.05978577211499214\n","Epoch: 1 / 2, Steps: 21501 / 55237, Loss: 0.09665798395872116\n","Epoch: 1 / 2, Steps: 22001 / 55237, Loss: 0.10426048189401627\n","Epoch: 1 / 2, Steps: 22501 / 55237, Loss: 0.024327848106622696\n","Epoch: 1 / 2, Steps: 23001 / 55237, Loss: 0.059278491884469986\n","Epoch: 1 / 2, Steps: 23501 / 55237, Loss: 0.04403792694211006\n","Epoch: 1 / 2, Steps: 24001 / 55237, Loss: 0.01563686691224575\n","Epoch: 1 / 2, Steps: 24501 / 55237, Loss: 0.11985527724027634\n","Epoch: 1 / 2, Steps: 25001 / 55237, Loss: 0.18718302249908447\n","Epoch: 1 / 2, Steps: 25501 / 55237, Loss: 0.0862676203250885\n","Epoch: 1 / 2, Steps: 26001 / 55237, Loss: 0.12863005697727203\n","Epoch: 1 / 2, Steps: 26501 / 55237, Loss: 0.07455230504274368\n","Epoch: 1 / 2, Steps: 27001 / 55237, Loss: 0.031200869008898735\n","Epoch: 1 / 2, Steps: 27501 / 55237, Loss: 0.035984817892313004\n","Epoch: 1 / 2, Steps: 28001 / 55237, Loss: 0.01315278559923172\n","Epoch: 1 / 2, Steps: 28501 / 55237, Loss: 0.05548219755291939\n","Epoch: 1 / 2, Steps: 29001 / 55237, Loss: 0.09773282706737518\n","Epoch: 1 / 2, Steps: 29501 / 55237, Loss: 0.023204533383250237\n","Epoch: 1 / 2, Steps: 30001 / 55237, Loss: 0.040839213877916336\n","Epoch: 1 / 2, Steps: 30501 / 55237, Loss: 0.017294209450483322\n","Epoch: 1 / 2, Steps: 31001 / 55237, Loss: 0.07979486882686615\n","Epoch: 1 / 2, Steps: 31501 / 55237, Loss: 0.14217296242713928\n","Epoch: 1 / 2, Steps: 32001 / 55237, Loss: 0.0884726345539093\n","Epoch: 1 / 2, Steps: 32501 / 55237, Loss: 0.06765355169773102\n","Epoch: 1 / 2, Steps: 33001 / 55237, Loss: 0.014962935820221901\n","Epoch: 1 / 2, Steps: 33501 / 55237, Loss: 0.05353167653083801\n","Epoch: 1 / 2, Steps: 34001 / 55237, Loss: 0.056636884808540344\n","Epoch: 1 / 2, Steps: 34501 / 55237, Loss: 0.010141926817595959\n","Epoch: 1 / 2, Steps: 35001 / 55237, Loss: 0.05734900385141373\n","Epoch: 1 / 2, Steps: 35501 / 55237, Loss: 0.07997105270624161\n","Epoch: 1 / 2, Steps: 36001 / 55237, Loss: 0.0948052778840065\n","Epoch: 1 / 2, Steps: 36501 / 55237, Loss: 0.05329481139779091\n","Epoch: 1 / 2, Steps: 37001 / 55237, Loss: 0.01086339820176363\n","Epoch: 1 / 2, Steps: 37501 / 55237, Loss: 0.013931414112448692\n","Epoch: 1 / 2, Steps: 38001 / 55237, Loss: 0.014886945486068726\n","Epoch: 1 / 2, Steps: 38501 / 55237, Loss: 0.013016382232308388\n","Epoch: 1 / 2, Steps: 39001 / 55237, Loss: 0.01683051325380802\n","Epoch: 1 / 2, Steps: 39501 / 55237, Loss: 0.11155043542385101\n","Epoch: 1 / 2, Steps: 40001 / 55237, Loss: 0.09400837868452072\n","Epoch: 1 / 2, Steps: 40501 / 55237, Loss: 0.0971253365278244\n","Epoch: 1 / 2, Steps: 41001 / 55237, Loss: 0.015480521135032177\n","Epoch: 1 / 2, Steps: 41501 / 55237, Loss: 0.09246218949556351\n","Epoch: 1 / 2, Steps: 42001 / 55237, Loss: 0.10865269601345062\n","Epoch: 1 / 2, Steps: 42501 / 55237, Loss: 0.09043983370065689\n","Epoch: 1 / 2, Steps: 43001 / 55237, Loss: 0.06023985892534256\n","Epoch: 1 / 2, Steps: 43501 / 55237, Loss: 0.14998362958431244\n","Epoch: 1 / 2, Steps: 44001 / 55237, Loss: 0.014825811609625816\n","Epoch: 1 / 2, Steps: 44501 / 55237, Loss: 0.012628831900656223\n","Epoch: 1 / 2, Steps: 45001 / 55237, Loss: 0.2016516923904419\n","Epoch: 1 / 2, Steps: 45501 / 55237, Loss: 0.052238840609788895\n","Epoch: 1 / 2, Steps: 46001 / 55237, Loss: 0.018717994913458824\n","Epoch: 1 / 2, Steps: 46501 / 55237, Loss: 0.09391696751117706\n","Epoch: 1 / 2, Steps: 47001 / 55237, Loss: 0.016712045297026634\n","Epoch: 1 / 2, Steps: 47501 / 55237, Loss: 0.053564734756946564\n","Epoch: 1 / 2, Steps: 48001 / 55237, Loss: 0.02494499273598194\n","Epoch: 1 / 2, Steps: 48501 / 55237, Loss: 0.10995230823755264\n","Epoch: 1 / 2, Steps: 49001 / 55237, Loss: 0.05911308899521828\n","Epoch: 1 / 2, Steps: 49501 / 55237, Loss: 0.07011162489652634\n","Epoch: 1 / 2, Steps: 50001 / 55237, Loss: 0.07271009683609009\n","Epoch: 1 / 2, Steps: 50501 / 55237, Loss: 0.054395366460084915\n","Epoch: 1 / 2, Steps: 51001 / 55237, Loss: 0.12022066861391068\n","Epoch: 1 / 2, Steps: 51501 / 55237, Loss: 0.09882774949073792\n","Epoch: 1 / 2, Steps: 52001 / 55237, Loss: 0.014399555511772633\n","Epoch: 1 / 2, Steps: 52501 / 55237, Loss: 0.13439999520778656\n","Epoch: 1 / 2, Steps: 53001 / 55237, Loss: 0.1649717092514038\n","Epoch: 1 / 2, Steps: 53501 / 55237, Loss: 0.015414443798363209\n","Epoch: 1 / 2, Steps: 54001 / 55237, Loss: 0.05681677907705307\n","Epoch: 1 / 2, Steps: 54501 / 55237, Loss: 0.01720893196761608\n","Epoch: 1 / 2, Steps: 55001 / 55237, Loss: 0.012774971313774586\n","Epoch: 2 / 2, Steps: 1 / 55237, Loss: 0.04142985865473747\n","Epoch: 2 / 2, Steps: 501 / 55237, Loss: 0.05098816379904747\n","Epoch: 2 / 2, Steps: 1001 / 55237, Loss: 0.03519317880272865\n","Epoch: 2 / 2, Steps: 1501 / 55237, Loss: 0.05729948356747627\n","Epoch: 2 / 2, Steps: 2001 / 55237, Loss: 0.009080139920115471\n","Epoch: 2 / 2, Steps: 2501 / 55237, Loss: 0.011611309833824635\n","Epoch: 2 / 2, Steps: 3001 / 55237, Loss: 0.06356373429298401\n","Epoch: 2 / 2, Steps: 3501 / 55237, Loss: 0.0497441440820694\n","Epoch: 2 / 2, Steps: 4001 / 55237, Loss: 0.030331704765558243\n","Epoch: 2 / 2, Steps: 4501 / 55237, Loss: 0.014149249531328678\n","Epoch: 2 / 2, Steps: 5001 / 55237, Loss: 0.013938435353338718\n","Epoch: 2 / 2, Steps: 5501 / 55237, Loss: 0.08485367149114609\n","Epoch: 2 / 2, Steps: 6001 / 55237, Loss: 0.05483543872833252\n","Epoch: 2 / 2, Steps: 6501 / 55237, Loss: 0.0101114297285676\n","Epoch: 2 / 2, Steps: 7001 / 55237, Loss: 0.09608879685401917\n","Epoch: 2 / 2, Steps: 7501 / 55237, Loss: 0.13643479347229004\n","Epoch: 2 / 2, Steps: 8001 / 55237, Loss: 0.012043087743222713\n","Epoch: 2 / 2, Steps: 8501 / 55237, Loss: 0.013175878673791885\n","Epoch: 2 / 2, Steps: 9001 / 55237, Loss: 0.09711803495883942\n","Epoch: 2 / 2, Steps: 9501 / 55237, Loss: 0.017776615917682648\n","Epoch: 2 / 2, Steps: 10001 / 55237, Loss: 0.053789172321558\n","Epoch: 2 / 2, Steps: 10501 / 55237, Loss: 0.04665558040142059\n","Epoch: 2 / 2, Steps: 11001 / 55237, Loss: 0.07883361726999283\n","Epoch: 2 / 2, Steps: 11501 / 55237, Loss: 0.04643737152218819\n","Epoch: 2 / 2, Steps: 12001 / 55237, Loss: 0.05278966575860977\n","Epoch: 2 / 2, Steps: 12501 / 55237, Loss: 0.1291242390871048\n","Epoch: 2 / 2, Steps: 13001 / 55237, Loss: 0.009283492341637611\n","Epoch: 2 / 2, Steps: 13501 / 55237, Loss: 0.05631496012210846\n","Epoch: 2 / 2, Steps: 14001 / 55237, Loss: 0.020355751737952232\n","Epoch: 2 / 2, Steps: 14501 / 55237, Loss: 0.06217794492840767\n","Epoch: 2 / 2, Steps: 15001 / 55237, Loss: 0.015401623211801052\n","Epoch: 2 / 2, Steps: 15501 / 55237, Loss: 0.040599312633275986\n","Epoch: 2 / 2, Steps: 16001 / 55237, Loss: 0.05670978128910065\n","Epoch: 2 / 2, Steps: 16501 / 55237, Loss: 0.01033017411828041\n","Epoch: 2 / 2, Steps: 17001 / 55237, Loss: 0.06009596213698387\n","Epoch: 2 / 2, Steps: 17501 / 55237, Loss: 0.018923474475741386\n","Epoch: 2 / 2, Steps: 18001 / 55237, Loss: 0.05046701803803444\n","Epoch: 2 / 2, Steps: 18501 / 55237, Loss: 0.050454724580049515\n","Epoch: 2 / 2, Steps: 19001 / 55237, Loss: 0.021788569167256355\n","Epoch: 2 / 2, Steps: 19501 / 55237, Loss: 0.1260707527399063\n","Epoch: 2 / 2, Steps: 20001 / 55237, Loss: 0.08572966605424881\n","Epoch: 2 / 2, Steps: 20501 / 55237, Loss: 0.08856017142534256\n","Epoch: 2 / 2, Steps: 21001 / 55237, Loss: 0.06058260053396225\n","Epoch: 2 / 2, Steps: 21501 / 55237, Loss: 0.06993935257196426\n","Epoch: 2 / 2, Steps: 22001 / 55237, Loss: 0.14574386179447174\n","Epoch: 2 / 2, Steps: 22501 / 55237, Loss: 0.09824171662330627\n","Epoch: 2 / 2, Steps: 23001 / 55237, Loss: 0.012575246393680573\n","Epoch: 2 / 2, Steps: 23501 / 55237, Loss: 0.010429849848151207\n","Epoch: 2 / 2, Steps: 24001 / 55237, Loss: 0.01091613620519638\n","Epoch: 2 / 2, Steps: 24501 / 55237, Loss: 0.10610860586166382\n","Epoch: 2 / 2, Steps: 25001 / 55237, Loss: 0.14617116749286652\n","Epoch: 2 / 2, Steps: 25501 / 55237, Loss: 0.14150306582450867\n","Epoch: 2 / 2, Steps: 26001 / 55237, Loss: 0.050252918154001236\n","Epoch: 2 / 2, Steps: 26501 / 55237, Loss: 0.01126832701265812\n","Epoch: 2 / 2, Steps: 27001 / 55237, Loss: 0.11101776361465454\n","Epoch: 2 / 2, Steps: 27501 / 55237, Loss: 0.048961807042360306\n","Epoch: 2 / 2, Steps: 28001 / 55237, Loss: 0.02966572344303131\n","Epoch: 2 / 2, Steps: 28501 / 55237, Loss: 0.054575029760599136\n","Epoch: 2 / 2, Steps: 29001 / 55237, Loss: 0.15809136629104614\n","Epoch: 2 / 2, Steps: 29501 / 55237, Loss: 0.10799416899681091\n","Epoch: 2 / 2, Steps: 30001 / 55237, Loss: 0.05108516663312912\n","Epoch: 2 / 2, Steps: 30501 / 55237, Loss: 0.10587599873542786\n","Epoch: 2 / 2, Steps: 31001 / 55237, Loss: 0.011427542194724083\n","Epoch: 2 / 2, Steps: 31501 / 55237, Loss: 0.009992288425564766\n","Epoch: 2 / 2, Steps: 32001 / 55237, Loss: 0.09294747561216354\n","Epoch: 2 / 2, Steps: 32501 / 55237, Loss: 0.07109903544187546\n","Epoch: 2 / 2, Steps: 33001 / 55237, Loss: 0.05136490985751152\n","Epoch: 2 / 2, Steps: 33501 / 55237, Loss: 0.14445525407791138\n","Epoch: 2 / 2, Steps: 34001 / 55237, Loss: 0.09187320619821548\n","Epoch: 2 / 2, Steps: 34501 / 55237, Loss: 0.141457200050354\n","Epoch: 2 / 2, Steps: 35001 / 55237, Loss: 0.012534038163721561\n","Epoch: 2 / 2, Steps: 35501 / 55237, Loss: 0.04977995529770851\n","Epoch: 2 / 2, Steps: 36001 / 55237, Loss: 0.012169433757662773\n","Epoch: 2 / 2, Steps: 36501 / 55237, Loss: 0.008855507709085941\n","Epoch: 2 / 2, Steps: 37001 / 55237, Loss: 0.099884994328022\n","Epoch: 2 / 2, Steps: 37501 / 55237, Loss: 0.009362399578094482\n","Epoch: 2 / 2, Steps: 38001 / 55237, Loss: 0.08791396021842957\n","Epoch: 2 / 2, Steps: 38501 / 55237, Loss: 0.06916995346546173\n","Epoch: 2 / 2, Steps: 39001 / 55237, Loss: 0.06345373392105103\n","Epoch: 2 / 2, Steps: 39501 / 55237, Loss: 0.010833612643182278\n","Epoch: 2 / 2, Steps: 40001 / 55237, Loss: 0.1033979058265686\n","Epoch: 2 / 2, Steps: 40501 / 55237, Loss: 0.01407388225197792\n","Epoch: 2 / 2, Steps: 41001 / 55237, Loss: 0.0296176765114069\n","Epoch: 2 / 2, Steps: 41501 / 55237, Loss: 0.059290479868650436\n","Epoch: 2 / 2, Steps: 42001 / 55237, Loss: 0.07881190627813339\n","Epoch: 2 / 2, Steps: 42501 / 55237, Loss: 0.025736356154084206\n","Epoch: 2 / 2, Steps: 43001 / 55237, Loss: 0.09664695709943771\n","Epoch: 2 / 2, Steps: 43501 / 55237, Loss: 0.009790337644517422\n","Epoch: 2 / 2, Steps: 44001 / 55237, Loss: 0.03870607912540436\n","Epoch: 2 / 2, Steps: 44501 / 55237, Loss: 0.029437048360705376\n","Epoch: 2 / 2, Steps: 45001 / 55237, Loss: 0.1950138807296753\n","Epoch: 2 / 2, Steps: 45501 / 55237, Loss: 0.0572933629155159\n","Epoch: 2 / 2, Steps: 46001 / 55237, Loss: 0.013045373372733593\n","Epoch: 2 / 2, Steps: 46501 / 55237, Loss: 0.03956286609172821\n","Epoch: 2 / 2, Steps: 47001 / 55237, Loss: 0.07358237355947495\n","Epoch: 2 / 2, Steps: 47501 / 55237, Loss: 0.009256983175873756\n","Epoch: 2 / 2, Steps: 48001 / 55237, Loss: 0.09388937801122665\n","Epoch: 2 / 2, Steps: 48501 / 55237, Loss: 0.05295233428478241\n","Epoch: 2 / 2, Steps: 49001 / 55237, Loss: 0.010757285170257092\n","Epoch: 2 / 2, Steps: 49501 / 55237, Loss: 0.09837235510349274\n","Epoch: 2 / 2, Steps: 50001 / 55237, Loss: 0.056867800652980804\n","Epoch: 2 / 2, Steps: 50501 / 55237, Loss: 0.05754702538251877\n","Epoch: 2 / 2, Steps: 51001 / 55237, Loss: 0.010434586554765701\n","Epoch: 2 / 2, Steps: 51501 / 55237, Loss: 0.05448666214942932\n","Epoch: 2 / 2, Steps: 52001 / 55237, Loss: 0.01213188748806715\n","Epoch: 2 / 2, Steps: 52501 / 55237, Loss: 0.05599850043654442\n","Epoch: 2 / 2, Steps: 53001 / 55237, Loss: 0.06657908111810684\n","Epoch: 2 / 2, Steps: 53501 / 55237, Loss: 0.07231807708740234\n","Epoch: 2 / 2, Steps: 54001 / 55237, Loss: 0.05099755898118019\n","Epoch: 2 / 2, Steps: 54501 / 55237, Loss: 0.0533457025885582\n","Epoch: 2 / 2, Steps: 55001 / 55237, Loss: 0.03167737275362015\n","Modèle entraîné sur l'ensemble de données d'entraînement.\n"]}],"source":["# Initialiser le writer de Tensorboard pour enregistrer les métriques d'entraînement\n","writer = SummaryWriter()\n","print(\"Writer de Tensorboard initialisé.\")\n","\n","# Entraîner le modèle sur l'ensemble de données d'entraînement\n","max_iter = 2\n","train_model(model, criterion, optimizer, trainloader, max_iter, writer, device)\n","print(\"Modèle entraîné sur l'ensemble de données d'entraînement.\")"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Evaluation du modèle"]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2024-04-11T02:38:06.169767Z","iopub.status.busy":"2024-04-11T02:38:06.168877Z","iopub.status.idle":"2024-04-11T02:41:57.673068Z","shell.execute_reply":"2024-04-11T02:41:57.672090Z","shell.execute_reply.started":"2024-04-11T02:38:06.169734Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Métriques d'évaluation:\n","Test set loss: 0.0526\n","Accuracy means: 0.9884\n","Precision: 0.9491\n","Recall: 0.9525\n","F1-score: 0.9506\n","Modèle évalué sur l'ensemble de données de test.\n","Writer de Tensorboard fermé.\n"]}],"source":["# Nombre de classes\n","Nb_class = len(df['Label'].unique())\n","\n","# Évaluer le modèle sur l'ensemble de données de test et afficher les métriques\n","metrics = evaluate_model(model, criterion, testloader, device, Nb_class)\n","\n","print(\"Métriques d'évaluation:\")\n","for metric_name, value in metrics.items():\n","    print(f\"{metric_name}: {value:.4f}\")\n","print(\"Modèle évalué sur l'ensemble de données de test.\")\n","\n","# Fermer le writer de Tensorboard\n","writer.close()\n","print(\"Writer de Tensorboard fermé.\")"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":902298,"sourceId":1530359,"sourceType":"datasetVersion"}],"dockerImageVersionId":30684,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
